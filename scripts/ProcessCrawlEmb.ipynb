{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from contextlib import contextmanager\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[crawl] done in 84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with timer('crawl'):\n",
    "    crawl_emb_dict = load_embeddings(CRAWL_EMBEDDING_PATH)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.69 s, sys: 713 ms, total: 9.4 s\n",
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 9.9G\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "x_train = train['comment_text']\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test['comment_text']\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# これだと、'はembeddingに結構入ってるのに除外されちゃう。　よくないので ' だけ抜いた\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "puncts_apos = [\"''\", ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "def clean_text_apos(x: str) -> str:\n",
    "    for punct in puncts_apos:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Dict, List\n",
    "def build_vocab(texts: pd.DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    -----\n",
    "    texts: pandas.Series\n",
    "        question textの列\n",
    "        \n",
    "    Returns\n",
    "    -----\n",
    "    dict: \n",
    "        単語とカウント\n",
    "    \n",
    "    \"\"\"\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab: Dict[str, int], embeddings_index: Dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----\n",
    "    vocab: dict\n",
    "        単語とカウント\n",
    "    embeddings_index: dict\n",
    "        load_embedの出力\n",
    "        \n",
    "    Returns:\n",
    "        list:\n",
    "            embeddingsに入ってない単語\n",
    "    \"\"\"\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / float(len(vocab))))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(float(nb_known_words) / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "l_stemmer = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    wordの編集距離1の単語のリストを返す\n",
    "    \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words, embed): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in embed)\n",
    "\n",
    "def spellcheck(word, word_rank_dict):\n",
    "    return min(known(edits1(word), word_rank_dict), key=lambda w: word_rank_dict[w])\n",
    "\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "import unicodedata\n",
    "def process_stemmer(vocab, embed):\n",
    "    \n",
    "    oov_word_set = set()\n",
    "    for word in vocab.keys():\n",
    "        vector = embed.get(word, None)\n",
    "        if vector is not None:\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.lower(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.upper(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.capitalize(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        corr_word = punct_mapping.get(word, None)\n",
    "        if corr_word is not None:\n",
    "            vector = embed.get(corr_word, None)\n",
    "            if vector is not None:\n",
    "                embed[word] = vector\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(p_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(p_stemmer.stem(word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(l_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(l_stemmer.stem(word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(s_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(s_stemmer.stem(word.decode('utf-8')), None)\n",
    "                    \n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        char_list = []\n",
    "        any_small_capitial = False\n",
    "        for char in word:\n",
    "            try:\n",
    "                uni_name = unicodedata.name(char)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            if 'LATIN LETTER SMALL CAPITAL' in uni_name:\n",
    "                char = uni_name[-1]\n",
    "                any_small_capitial = True\n",
    "            if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:\n",
    "                char = 'F'\n",
    "                any_small_capitial = True\n",
    "                \n",
    "            char_list.append(char)\n",
    "            \n",
    "        if not any_small_capitial:\n",
    "            oov_word_set.add(word)\n",
    "            continue\n",
    "        \n",
    "        legit_word = ''.join(char_list)\n",
    "        \n",
    "        # 2週目\n",
    "        \n",
    "        vector = embed.get(legit_word, None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(legit_word.lower(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(legit_word.upper(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(legit_word.capitalize(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        corr_word = punct_mapping.get(legit_word, None)\n",
    "        if corr_word is not None:\n",
    "            vector = embed.get(corr_word, None)\n",
    "            if vector is not None:\n",
    "                embed[word] = vector\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(p_stemmer.stem(legit_word), None)\n",
    "        except:\n",
    "            vector = embed.get(p_stemmer.stem(legit_word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(l_stemmer.stem(legit_word), None)\n",
    "        except:\n",
    "            vector = embed.get(l_stemmer.stem(legit_word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(s_stemmer.stem(legit_word), None)\n",
    "        except:\n",
    "            vector = embed.get(s_stemmer.stem(legit_word.decode('utf-8')), None)\n",
    "                    \n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        oov_word_set.add(word)\n",
    "            \n",
    "    return embed, oov_word_set\n",
    "\n",
    "def process_spellcheck(vocab, embed, word_rank_dict, oov_set):\n",
    "    for word in vocab.keys():\n",
    "        if word not in oov_set:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(spellcheck(word, word_rank_dict), None)\n",
    "        except:\n",
    "            continue\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "    return embed\n",
    "\n",
    "def make_word_rank(embed):\n",
    "    word_rank = {}\n",
    "    for i, word in enumerate(embed):\n",
    "        word_rank[word] = i\n",
    "    return word_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_desc = pd.concat([x_train, x_test], ignore_index=True).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 260 ms, total: 12 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_concat_desc = concat_desc.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 s, sys: 1.22 s, total: 28.8 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = build_vocab(processed_concat_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 53.24% of vocab\n",
      "Found embeddings for  98.91% of all text\n",
      "CPU times: user 353 ms, sys: 19.7 ms, total: 373 ms\n",
      "Wall time: 371 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oov = check_coverage(vocab, crawl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 65327),\n",
       " (\"Trump's\", 25361),\n",
       " (\"aren't\", 22714),\n",
       " (\"Don't\", 21779),\n",
       " (\"wouldn't\", 21158),\n",
       " (\"wasn't\", 19896),\n",
       " (\"You're\", 14954),\n",
       " (\"Let's\", 14817),\n",
       " (\"He's\", 12654),\n",
       " (\"couldn't\", 12060),\n",
       " (\"There's\", 11290),\n",
       " (\"let's\", 10409),\n",
       " (\"what's\", 10359),\n",
       " (\"shouldn't\", 10267),\n",
       " (\"hasn't\", 8539),\n",
       " (\"What's\", 8478),\n",
       " (\"Canada's\", 8409),\n",
       " (\"you've\", 8138),\n",
       " ('`', 7591),\n",
       " (\"weren't\", 6634),\n",
       " (\"Here's\", 6307),\n",
       " (\"Obama's\", 6232),\n",
       " (\"They're\", 5807),\n",
       " (\"one's\", 5607),\n",
       " (\"people's\", 5597),\n",
       " (\"you'd\", 5440),\n",
       " (\"we'll\", 5293),\n",
       " (\"they've\", 5179),\n",
       " (\"We're\", 5156),\n",
       " (\"Can't\", 4993),\n",
       " (\"they'll\", 4990),\n",
       " (\"we've\", 4944),\n",
       " (\"today's\", 4798),\n",
       " (\"Trudeau's\", 4645),\n",
       " (\"who's\", 4599),\n",
       " (\"Isn't\", 4441),\n",
       " (\"Alaska's\", 4127),\n",
       " (\"God's\", 3650),\n",
       " (\"he'll\", 3471),\n",
       " (\"ain't\", 3241),\n",
       " (\"women's\", 3182),\n",
       " (\"Didn't\", 3120),\n",
       " (\"Doesn't\", 3119),\n",
       " (\"they'd\", 3073),\n",
       " (\"She's\", 3060),\n",
       " (\"world's\", 3032),\n",
       " (\"America's\", 2962),\n",
       " (\"he'd\", 2908),\n",
       " (\"Clinton's\", 2901),\n",
       " (\"You've\", 2833),\n",
       " (\"We've\", 2758),\n",
       " (\"else's\", 2526),\n",
       " (\"Hillary's\", 2487),\n",
       " (\"we'd\", 2335),\n",
       " (\"gov't\", 2309),\n",
       " (\"country's\", 2288),\n",
       " (\"We'll\", 2278),\n",
       " (\"O'Leary\", 2190),\n",
       " (\"hadn't\", 2153),\n",
       " (\"state's\", 2145),\n",
       " (\"man's\", 2101),\n",
       " (\"government's\", 2092),\n",
       " (\"person's\", 2087),\n",
       " (\"someone's\", 2054),\n",
       " (\"Harper's\", 1946),\n",
       " ('SB21', 1936),\n",
       " (\"here's\", 1885),\n",
       " (\"nation's\", 1828),\n",
       " (\"You'll\", 1818),\n",
       " (\"They've\", 1744),\n",
       " (\"everyone's\", 1657),\n",
       " (\"Wouldn't\", 1637),\n",
       " (\"Where's\", 1632),\n",
       " (\"Hawaii's\", 1629),\n",
       " (\"Putin's\", 1573),\n",
       " (\"it'll\", 1504),\n",
       " (\"China's\", 1488),\n",
       " ('theglobeandmail', 1420),\n",
       " (\"woman's\", 1396),\n",
       " (\"You'd\", 1382),\n",
       " (\"Church's\", 1365),\n",
       " (\"Who's\", 1291),\n",
       " (\"They'll\", 1271),\n",
       " (\"i'm\", 1270),\n",
       " (\"children's\", 1258),\n",
       " (\"anyone's\", 1230),\n",
       " (\"year's\", 1229),\n",
       " (\"Hawai'i\", 1226),\n",
       " (\"60's\", 1197),\n",
       " (\"He'll\", 1184),\n",
       " (\"Shouldn't\", 1150),\n",
       " (\"70's\", 1143),\n",
       " (\"80's\", 1117),\n",
       " (\"would've\", 1116),\n",
       " (\"How's\", 1099),\n",
       " (\"Wasn't\", 1095),\n",
       " (\"Russia's\", 1091),\n",
       " (\"Oregon's\", 1085),\n",
       " (\"Aren't\", 1068),\n",
       " (\"BC's\", 1067)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 84.36% of vocab\n",
      "Found embeddings for  99.87% of all text\n",
      "CPU times: user 32.9 s, sys: 44 ms, total: 32.9 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crawl_emb_dict, oov = process_stemmer(vocab, crawl_emb_dict)\n",
    "word_rank = make_word_rank(crawl_emb_dict)\n",
    "crawl_emb_dict = process_spellcheck(vocab, crawl_emb_dict, word_rank, oov)\n",
    "oov = check_coverage(vocab, crawl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theglobeandmail', 1420),\n",
       " ('nationalpost', 438),\n",
       " ('2gTbpns', 381),\n",
       " ('denverpost', 339),\n",
       " ('civilbeat', 285),\n",
       " ('RangerMC', 276),\n",
       " ('garycrum', 266),\n",
       " ('BCLibs', 260),\n",
       " ('cashapp24', 237),\n",
       " ('dailycaller', 217),\n",
       " ('washingtontimes', 210),\n",
       " ('Cheetolini', 203),\n",
       " ('Tridentinus', 201),\n",
       " ('Ontariowe', 184),\n",
       " ('financialpost', 179),\n",
       " ('MAGAphants', 178),\n",
       " ('Nageak', 173),\n",
       " (\"O'Leary's\", 170),\n",
       " ('scientificamerican', 161),\n",
       " ('motleycrew', 161),\n",
       " ('907AK', 158),\n",
       " ('ncronline', 158),\n",
       " ('talkingpointsmemo', 155),\n",
       " ('motherjones', 153),\n",
       " ('Outsider77', 151),\n",
       " ('Putrumpski', 150),\n",
       " ('diverdave', 148),\n",
       " ('Mahawker', 148),\n",
       " ('TheDonald', 140),\n",
       " ('antifluoridationists', 140),\n",
       " ('staradvertiser', 133),\n",
       " ('Lazeelink', 131),\n",
       " ('Pandora17', 128),\n",
       " ('22moneybay', 125),\n",
       " ('Bozievich', 125),\n",
       " ('thedailybeast', 122),\n",
       " ('covfefe', 121),\n",
       " ('RadirD', 121),\n",
       " ('skyofblue', 121),\n",
       " ('gubmut', 119),\n",
       " ('muckamuck', 118),\n",
       " ('conservativereview', 115),\n",
       " ('americanthinker', 115),\n",
       " ('vancouversun', 114),\n",
       " ('McWynnety', 114),\n",
       " ('jerry69', 113),\n",
       " ('torontosun', 111),\n",
       " ('politicususa', 109),\n",
       " ('Colkoch', 107),\n",
       " ('antifluoridationist', 107),\n",
       " (\"Gabbard's\", 107),\n",
       " ('seattletimes', 106),\n",
       " (\"ponokeali'i\", 104),\n",
       " ('pewresearch', 101),\n",
       " ('\\xadc\\xado\\xadm', 101),\n",
       " ('w\\xadw\\xadw\\xad', 97),\n",
       " ('Patkotak', 97),\n",
       " ('clickSource', 96),\n",
       " ('Whazzie', 95),\n",
       " ('jjp58', 94),\n",
       " ('MAGAphant', 92),\n",
       " ('cn535aU5UsN7Lj8X8', 91),\n",
       " ('MNXHNMJYAz1b41', 91),\n",
       " ('1DzOz3Y6D8g', 91),\n",
       " ('crooksandliars', 91),\n",
       " ('Trumplethinskin', 90),\n",
       " ('dtrumpview', 89),\n",
       " ('factoryofincome', 89),\n",
       " ('AlwaysThere', 89),\n",
       " ('Rolovich', 87),\n",
       " ('Gegonos', 87),\n",
       " ('theDonald', 86),\n",
       " ('cruxnow', 86),\n",
       " ('mcubz', 85),\n",
       " ('Trumpnuts', 85),\n",
       " ('WesternPatriot', 83),\n",
       " ('Utqiagvik', 83),\n",
       " ('3Ahomepage', 83),\n",
       " ('altrightpubs', 82),\n",
       " ('dailywire', 82),\n",
       " ('StewartBrian', 81),\n",
       " ('s\\xadp\\xado\\xadt\\xad', 81),\n",
       " ('\\xadc\\xada\\xads\\xadh\\xad', 81),\n",
       " ('Veselnitskaya', 80),\n",
       " ('Mokantx', 80),\n",
       " ('AnonAJ', 80),\n",
       " ('nationalobserver', 80),\n",
       " ('NFLGTV', 78),\n",
       " ('libertyheadlines', 78),\n",
       " ('oldgit', 78),\n",
       " ('secondenlightenment', 76),\n",
       " ('indivisibleguide', 75),\n",
       " ('cbslocal', 75),\n",
       " ('RAILFAIL', 74),\n",
       " ('EuGreen', 74),\n",
       " ('Sniktaw', 73),\n",
       " ('Trumpski', 72),\n",
       " ('ericnorstog', 72),\n",
       " ('Eugreen', 72),\n",
       " ('bsdetection', 71)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.9900e-02, -6.0200e-01, -1.2310e-01, -5.6850e-01,  3.8700e-02,\n",
       "       -9.7500e-02,  3.6540e-01,  2.3150e-01, -1.0831e+00,  1.6100e-02,\n",
       "       -1.6860e-01,  2.8690e-01, -6.5800e-02,  8.4400e-02, -1.3490e-01,\n",
       "        1.1190e-01, -9.8720e-01,  1.2010e-01,  3.2180e-01,  8.1100e-02,\n",
       "       -6.7800e-02,  5.3900e-02, -8.5600e-02, -1.4750e-01, -2.0620e-01,\n",
       "        2.1600e-02,  1.6360e-01, -1.1330e-01,  1.3640e-01,  5.0080e-01,\n",
       "       -2.1800e-01, -7.1100e-02, -3.4560e-01, -5.9890e-01,  1.0000e-01,\n",
       "        3.4670e-01, -8.1900e-02, -2.7520e-01, -1.9740e-01,  6.1670e-01,\n",
       "        8.6100e-02,  1.9800e-02,  1.5950e-01,  5.8010e-01, -1.3000e-03,\n",
       "       -3.8500e-02,  1.4070e-01, -2.3600e-02,  3.1500e-02, -2.1140e-01,\n",
       "       -3.9000e-02,  4.4750e-01,  6.0760e-01, -2.3100e-01, -1.6230e-01,\n",
       "        1.1500e-02, -7.7400e-02,  1.0060e-01,  2.3250e-01,  3.4400e-02,\n",
       "        1.7830e-01, -1.0450e-01, -2.1590e-01, -2.6670e-01, -3.1260e-01,\n",
       "       -1.7290e-01, -1.8380e-01,  7.4600e-02,  1.7000e-02, -9.9500e-02,\n",
       "        1.1042e+00,  3.2550e-01, -3.8200e-02,  1.6600e-01,  1.1000e-01,\n",
       "       -7.8100e-02,  1.5230e-01,  1.5050e-01, -3.0700e-01, -1.2140e-01,\n",
       "        3.6010e-01, -1.4790e-01,  9.6600e-02,  8.0000e-03,  1.0560e-01,\n",
       "       -2.0000e-03, -9.0500e-02,  1.1910e-01, -1.3900e-02, -6.6800e-02,\n",
       "        1.4100e-01,  4.5060e-01, -8.4000e-03,  7.7800e-02, -2.6990e-01,\n",
       "        4.0970e-01, -3.4740e-01, -2.0370e-01, -1.8820e-01, -3.0440e-01,\n",
       "        1.1620e-01,  4.8000e-02,  6.2620e-01,  2.7040e-01, -7.2350e-01,\n",
       "       -3.3840e-01, -5.3100e-02, -1.2100e-02,  4.7900e-02,  1.3060e-01,\n",
       "       -4.2710e-01,  8.9200e-02, -1.6090e-01,  1.5270e-01, -5.4530e-01,\n",
       "        1.4680e-01, -2.9820e-01,  7.1300e-02, -1.5400e-01, -1.0740e-01,\n",
       "       -6.5470e-01,  1.0560e-01, -7.2200e-02, -1.4420e-01,  9.3000e-02,\n",
       "        1.6480e-01,  1.9400e-02,  1.5890e-01,  1.2020e-01,  1.4600e-02,\n",
       "        2.6400e-02,  4.8700e-02, -1.5960e-01, -1.1480e-01,  2.7420e-01,\n",
       "        3.0760e-01, -6.7890e-01, -8.2000e-03,  1.0380e-01,  3.2730e-01,\n",
       "        3.3890e-01, -4.8830e-01, -2.6880e-01,  5.4690e-01,  3.4070e-01,\n",
       "       -4.4000e-02,  1.3790e-01,  9.3500e-02,  2.7650e-01,  1.8120e-01,\n",
       "        7.9600e-02, -8.6600e-02,  1.8310e-01, -1.2700e-02,  1.1090e-01,\n",
       "       -7.4700e-02, -1.7960e-01,  2.2400e-02,  1.0080e-01,  1.1150e-01,\n",
       "        3.8410e-01,  1.7760e-01,  1.8730e-01,  4.6600e-02,  6.8400e-02,\n",
       "       -2.5100e-02,  3.6800e-02,  9.2400e-02,  7.7100e-02, -8.2600e-02,\n",
       "        5.5800e-02, -7.3800e-02,  1.3000e-03,  2.9890e-01, -2.9760e-01,\n",
       "       -2.0130e-01,  4.0600e-02, -1.0780e-01, -7.6600e-02, -8.3600e-02,\n",
       "        9.8600e-01,  1.6170e-01, -2.0600e-02, -1.2690e-01, -2.1720e-01,\n",
       "       -2.4260e-01, -4.1500e-02,  1.3210e-01, -2.7970e-01,  6.2900e-02,\n",
       "       -3.2170e-01,  1.1900e-01,  6.6670e-01,  1.1870e-01, -1.2480e-01,\n",
       "        1.8000e-03, -7.1000e-03,  1.1450e-01,  4.4000e-02, -7.4000e-03,\n",
       "       -3.8320e-01, -2.8460e-01,  3.5100e-02, -2.4430e-01, -2.5450e-01,\n",
       "       -1.2730e-01,  1.3200e-02, -1.3720e-01,  2.5200e-01,  3.9450e-01,\n",
       "        1.6480e-01,  1.5049e+00,  5.7000e-02, -2.1320e-01,  3.3960e-01,\n",
       "       -1.4600e-02, -4.7500e-02, -1.4180e-01,  9.8300e-02,  1.3880e-01,\n",
       "        4.5860e-01,  7.6400e-02, -7.5500e-02,  2.1020e-01,  1.5270e-01,\n",
       "        1.9450e-01,  1.4470e-01,  7.3500e-02,  2.0280e-01,  5.2500e-02,\n",
       "        2.0500e-02,  1.8080e-01, -4.5280e-01, -2.3240e-01,  2.6000e-02,\n",
       "       -8.5100e-02, -1.6750e-01,  1.2520e-01, -4.3390e-01, -1.0940e-01,\n",
       "       -4.3600e-02,  1.3060e-01,  2.3000e-02, -1.2700e-01, -1.7130e-01,\n",
       "        2.2100e-02,  1.1180e-01,  8.2000e-03,  2.2560e-01, -8.6900e-02,\n",
       "        3.0520e-01,  3.2400e-01,  1.1800e-01,  1.9400e-02, -5.2970e-01,\n",
       "        9.0700e-02,  6.8200e-02, -1.2220e-01, -2.5930e-01, -9.2300e-02,\n",
       "       -1.2380e-01,  6.1000e-02, -2.4510e-01, -1.7830e-01,  1.7860e-01,\n",
       "        2.5060e-01, -1.9150e-01,  6.9660e-01, -8.9400e-02, -2.7720e-01,\n",
       "       -3.3000e-03,  4.4900e-01,  1.6890e-01,  3.1790e-01,  1.8680e-01,\n",
       "        1.3400e-01, -5.9000e-03, -6.5200e-02,  5.9000e-02, -3.0200e-01,\n",
       "        1.8010e-01, -5.5280e-01,  2.8530e-01, -3.2840e-01,  2.6120e-01,\n",
       "       -7.1800e-02,  1.1820e-01,  1.2780e-01, -7.2100e-02,  3.0880e-01,\n",
       "       -9.8200e-02, -1.6100e-01, -1.0050e-01,  1.5300e-02, -8.3100e-02,\n",
       "       -6.0700e-01, -1.0450e-01, -2.5270e-01, -4.1600e-01, -3.0660e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_emb_dict['hasn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vocab\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# 7.67 -> 9.5くらい\n",
    "with open('../input/crawl_emb_processed.joblib', 'wb') as f:\n",
    "    joblib.dump(crawl_emb_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
