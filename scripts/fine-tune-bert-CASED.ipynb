{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASEDã§ã‚„ã‚‹\n",
    "\n",
    "batch sizeãŒå°ã•ã„ã®ã§ã€buckets ã¯ shuffleã—ãŸã»ã†ãŒã„ã„ã§ã—ã‚‡ã†\n",
    "\n",
    "Fine tuniningã™ã‚‹ï¼\n",
    "\n",
    "dropout 0.1ã™ã‚‹\n",
    "\n",
    "Tokenizeæ¸ˆã¿ã®trainã‚’ä½¿ã†\n",
    "\n",
    "MAX_LEN ã¯ 300ã§ã‚­ãƒ£ãƒƒãƒ—ã™ã‚‹\n",
    "\n",
    "sentence_dfã‚„ã‚‹, warm up\n",
    "\n",
    "\n",
    "# Property\n",
    "- LSTM\n",
    "- no annealing\n",
    "- aux\n",
    "- Subgroup negative\n",
    "\n",
    "## pytorch-pretrained-BERT\n",
    "- import pytorch-pretrained-bert from source in dataset\n",
    "\n",
    "the source is cloned from [huggingface/pytorch\\-pretrained\\-BERT: ðŸ“–The Big\\-&\\-Extending\\-Repository\\-of\\-Transformers: Pretrained PyTorch models for Google's BERT, OpenAI GPT & GPT\\-2, Google/CMU Transformer\\-XL\\.](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "\n",
    "- convert tf checkpoints to pytorch model\n",
    "\n",
    "\n",
    "- feature extraction with BERT like [huggingface/pytorch\\-pretrained\\-BERT: ðŸ“–The Big\\-&\\-Extending\\-Repository\\-of\\-Transformers: Pretrained PyTorch models for Google's BERT, OpenAI GPT & GPT\\-2, Google/CMU Transformer\\-XL\\.](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "\n",
    "This kernel is based off of [Import functions from Kaggle script](https://www.kaggle.com/rtatman/import-functions-from-kaggle-script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_seeds = 1\n",
    "n_splits = 10\n",
    "n_epochs = 3\n",
    "\n",
    "RESULT_TXT = f\"bert-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt\"\n",
    "RESULT_PATH = \"../models/fine-tune-bert-CASED\"\n",
    "\n",
    "OUT_DROPOUT = 0.1\n",
    "\n",
    "TRAIN_ON_N_SPLITS = 5\n",
    "\n",
    "SUBGROUP_NEGATIVE_WEIGHT_COEF = 1\n",
    "\n",
    "BERT_N_LAST_LAYER = 4\n",
    "BERT_HIDDEN_SIZE = 768\n",
    "\n",
    "BERT_MODEL_PATH = 'bert-base-cased'\n",
    "BERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n",
    "\n",
    "MAX_LEN = 220\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    DEBUG_DATA_SIZE = 10000\n",
    "    n_seeds = 1\n",
    "    n_splits = 10\n",
    "    n_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-20190502-004120.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULT_TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencefeaturesoov', 'crawl_emb_nocomp.pickle', 'jigsaw-unintended-bias-in-toxicity-classification', 'crawl_emb_processed_lz4.joblib', 'x-train-tokenized', 'crawl_emb_nocomp.joblib', 'crawl_emb_processed.joblib', 'bert-pretrained-models', 'fasttext-crawl-300d-2m', 'jigsaw-x-train-bert-tokenized', 'glove840b300dtxt', 'roov-crawl.pickle']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from contextlib import contextmanager\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.mkdir(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 1.04 s, total: 11.3 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "# ã“ã‚Œã ã¨ã€'ã¯embeddingã«çµæ§‹å…¥ã£ã¦ã‚‹ã®ã«é™¤å¤–ã•ã‚Œã¡ã‚ƒã†ã€‚ã€€ã‚ˆããªã„ã®ã§ ' ã ã‘æŠœã„ãŸ\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£', \n",
    " 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', \n",
    " 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', \n",
    " 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', \n",
    " 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš', ]\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "# 9.9G\n",
    "\n",
    "if DEBUG:\n",
    "    train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', nrows=DEBUG_DATA_SIZE)\n",
    "else:\n",
    "    train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    sentence_df = pd.read_csv('../input/sentencefeaturesoov/sentence_features.csv', nrows=DEBUG_DATA_SIZE)\n",
    "else:\n",
    "    sentence_df = pd.read_csv('../input/sentencefeaturesoov/sentence_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>n_ex</th>\n",
       "      <th>n_que</th>\n",
       "      <th>n_puncts</th>\n",
       "      <th>n_prof</th>\n",
       "      <th>n_oov</th>\n",
       "      <th>n_upper_ratio</th>\n",
       "      <th>n_unique_ratio</th>\n",
       "      <th>n_ex_ratio</th>\n",
       "      <th>n_que_ratio</th>\n",
       "      <th>n_puncts_ratio</th>\n",
       "      <th>n_prof_ratio</th>\n",
       "      <th>n_oov_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  n_upper  n_unique  n_ex  n_que  n_puncts  n_prof  n_oov  \\\n",
       "0          26        3        24     1      2         6       0      0   \n",
       "1          29        3        27     3      0         6       0      0   \n",
       "2          19        2        19     1      0         3       0      0   \n",
       "3          19        3        17     0      2         2       0      0   \n",
       "4           9        0         9     0      0         1       0      0   \n",
       "\n",
       "   n_upper_ratio  n_unique_ratio  n_ex_ratio  n_que_ratio  n_puncts_ratio  \\\n",
       "0       0.115385        0.923077    0.038462     0.076923        0.230769   \n",
       "1       0.103448        0.931034    0.103448     0.000000        0.206897   \n",
       "2       0.105263        1.000000    0.052632     0.000000        0.157895   \n",
       "3       0.157895        0.894737    0.000000     0.105263        0.105263   \n",
       "4       0.000000        1.000000    0.000000     0.000000        0.111111   \n",
       "\n",
       "   n_prof_ratio  n_oov_ratio  \n",
       "0           0.0          0.0  \n",
       "1           0.0          0.0  \n",
       "2           0.0          0.0  \n",
       "3           0.0          0.0  \n",
       "4           0.0          0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_feature_mat = sentence_df.values\n",
    "del sentence_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOXICITY_COLUMN = 'target'\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "subgroup_bool_train = train[identity_columns].fillna(0)>=0.5\n",
    "toxic_bool_train = train[TOXICITY_COLUMN].fillna(0)>=0.5\n",
    "subgroup_negative_mask = subgroup_bool_train.values.sum(axis=1).astype(bool) & ~toxic_bool_train\n",
    "\n",
    "sample_weight = np.ones((y_train.shape[0],))\n",
    "sample_weight += SUBGROUP_NEGATIVE_WEIGHT_COEF * subgroup_negative_mask\n",
    "\n",
    "del subgroup_bool_train, toxic_bool_train, subgroup_negative_mask\n",
    "gc.collect()\n",
    "\n",
    "y_train_torch = torch.tensor(np.concatenate([y_train[:, np.newaxis], y_aux_train, sample_weight[:, np.newaxis]], axis=1), dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.85 s, sys: 453 ms, total: 5.3 s\n",
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if BERT_DO_LOWER:\n",
    "    tokenized_path = '../input/jigsaw-x-train-bert-tokenized/x_train_tockenized.csv'\n",
    "else:\n",
    "    tokenized_path = '../input/jigsaw-x-train-bert-tokenized/x_train_tockenized_cased.csv'\n",
    "    \n",
    "\n",
    "if DEBUG:\n",
    "    df_x_tokenized = pd.read_csv(tokenized_path,\n",
    "                                 header=None, nrows=DEBUG_DATA_SIZE)\n",
    "else:\n",
    "    df_x_tokenized = pd.read_csv(tokenized_path,\n",
    "                                 header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS] This is so cool . It ' s like , ' would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS] Thank you ! ! This would make my life a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS] This is such an urgent design problem ; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CLS] Is this something I ' ll be able to inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CLS] ha ##ha you guys are a bunch of loser ##...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [CLS] This is so cool . It ' s like , ' would ...\n",
       "1  [CLS] Thank you ! ! This would make my life a ...\n",
       "2  [CLS] This is such an urgent design problem ; ...\n",
       "3  [CLS] Is this something I ' ll be able to inst...\n",
       "4  [CLS] ha ##ha you guys are a bunch of loser ##..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:13<00:00, 133794.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 1.38 s, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_tockenized = df_x_tokenized[0].progress_apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_x_tokenized\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[CLS], This, is, so, cool, ., It, ', s, like,...\n",
       "1    [[CLS], Thank, you, !, !, This, would, make, m...\n",
       "2    [[CLS], This, is, such, an, urgent, design, pr...\n",
       "3    [[CLS], Is, this, something, I, ', ll, be, abl...\n",
       "4    [[CLS], ha, ##ha, you, guys, are, a, bunch, of...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tockenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:29<00:00, 61350.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 1.03 s, total: 27.5 s\n",
      "Wall time: 30.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, do_lower_case=BERT_DO_LOWER)\n",
    "x_train_indexed = x_train_tockenized.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x[:MAX_LEN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train_tockenized, tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57802752"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BucketIterator(object):\n",
    "    def __init__(self, data, label, batch_size, pad_token, shuffle=True,\n",
    "                length_quantile=0.95):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_token = pad_token\n",
    "        self.length_quantile = length_quantile\n",
    "        # i_batch ã¨ i_bucketã‚’å¤‰æ›ã™ã‚‹index, shuffleãªã‚‰permuteã™ã‚‹\n",
    "        self.bucket_index = range(self.__len__())\n",
    "        \n",
    "        if not self.shuffle:\n",
    "            self.index_sorted = sorted(range(len(self.data)), key=lambda i: len(self.data[i]))\n",
    "        \n",
    "        self.reset_index()\n",
    "        \n",
    "    def reset_index(self):\n",
    "        self.i_batch = 0\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.index_sorted = sorted(np.random.permutation(len(self.data)), key=lambda i: len(self.data[i]))\n",
    "            self.bucket_index = np.random.permutation(self.__len__())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.batch_size + 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            i_bucket = self.bucket_index[self.i_batch]\n",
    "        except IndexError as e:\n",
    "            self.reset_index()\n",
    "            raise StopIteration\n",
    "            \n",
    "        index_batch = self.index_sorted[i_bucket * self.batch_size:\n",
    "                                        (i_bucket + 1) * self.batch_size]\n",
    "                                   \n",
    "        raw_batch_data = [self.data[i] for i in index_batch]\n",
    "        batch_label = self.label[index_batch]\n",
    "            \n",
    "        max_len = int(math.ceil(np.quantile([len(x) for x in raw_batch_data], self.length_quantile)))\n",
    "        segment_id_batch = np.zeros((len(raw_batch_data), max_len))\n",
    "        padded_batch = []\n",
    "        input_mask_batch = []\n",
    "        for sample in raw_batch_data:\n",
    "            input_mask = [1] * len(sample) + [0] * (max_len - len(sample))\n",
    "            input_mask_batch.append(input_mask[:max_len])\n",
    "            \n",
    "            sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n",
    "            padded_batch.append(sample[:max_len])\n",
    "            \n",
    "        self.i_batch += 1\n",
    "            \n",
    "        return padded_batch, segment_id_batch, input_mask_batch, batch_label, index_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_aux_targets, num_sentence_features):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.dropout = nn.Dropout(OUT_DROPOUT)\n",
    "        \n",
    "        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n",
    "        \n",
    "        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n",
    "        self.linear1 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        self.linear_out = nn.Linear(n_hidden, 1)\n",
    "        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x_features, sentence_features):\n",
    "        \n",
    "        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n",
    "        \n",
    "        bert_output = self.dropout(bert_output)\n",
    "        \n",
    "        h_sentence = self.linear_sentence1(sentence_features)\n",
    "        \n",
    "        h_cat = torch.cat((bert_output, h_sentence), 1)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear1(h_cat))\n",
    "        \n",
    "        hidden = h_cat + h_conc_linear1\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "OOF_TRAIN_COL = 'oof_train'\n",
    "SUBGROUP_AUC_COL = 'subgroup_auc'\n",
    "BPSN_AUC_COL = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC_COL = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "from sklearn import metrics\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup_col, label_col, oof_col):\n",
    "    subgroup_examples = df[df[subgroup_col]]\n",
    "    return compute_auc(subgroup_examples[label_col], subgroup_examples[oof_col])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup_col, label_col, oof_col):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup_col] & ~df[label_col]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup_col] & df[label_col]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label_col], examples[oof_col])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup_col, label_col, oof_col):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup_col] & df[label_col]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup_col] & ~df[label_col]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label_col], examples[oof_col])\n",
    "\n",
    "def compute_bias_metrics_for_model(df,\n",
    "                                   subgroup_list,\n",
    "                                   oof_col,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    record_list = []\n",
    "    for subgroup in subgroup_list:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(df[df[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC_COL] = compute_subgroup_auc(df, subgroup, label_col, oof_col)\n",
    "        record[BPSN_AUC_COL] = compute_bpsn_auc(df, subgroup, label_col, oof_col)\n",
    "        record[BNSP_AUC_COL] = compute_bnsp_auc(df, subgroup, label_col, oof_col)\n",
    "        record_list.append(record)\n",
    "    return pd.DataFrame(record_list).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "TOXICITY_COLUMN = 'target'\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# Convert taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC_COL], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC_COL], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC_COL], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "\n",
    "def get_various_auc(valid_df, y_pred):\n",
    "    valid_df = convert_dataframe_to_bool(valid_df.fillna(0))\n",
    "    valid_df.loc[:, OOF_TRAIN_COL] = y_pred\n",
    "    valid_df = convert_dataframe_to_bool(valid_df.fillna(0))\n",
    "    bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, OOF_TRAIN_COL, TOXICITY_COLUMN)\n",
    "    overall_auc = calculate_overall_auc(valid_df, OOF_TRAIN_COL)\n",
    "    return get_final_metric(bias_metrics_df, overall_auc), overall_auc, bias_metrics_df\n",
    "\n",
    "def adjust_lr(optimizer, i_batch, min_lr, max_lr, n_batch_all, warm_up_batch_ratio):\n",
    "    n_batch_warmed = int(n_batch_all * warm_up_batch_ratio)\n",
    "    if i_batch > n_batch_warmed:\n",
    "        optimizer.param_groups[0]['lr'] = max_lr\n",
    "    else:\n",
    "        optimizer.param_groups[0]['lr'] = (max_lr - min_lr) / n_batch_warmed * i_batch + min_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.clock()\n",
    "        self.old_time = time.clock()\n",
    "        print('start mearsure elapsed times')\n",
    "        \n",
    "    def stamp(self, comment):\n",
    "        print(comment + f': from start {time.clock() - self.start_time: .1f}, from old {self.old_time - - self.start_time: .1f}')\n",
    "        self.old_time = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57802752"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start mearsure elapsed times\n",
      "start seed 0\n",
      "seed start: from start  0.0, from old  163.6\n",
      "epoch start: from start  0.1, from old  163.6\n",
      "start fold 0\n",
      "toxic ratio dev: 0.17379875481128693, val: 0.1735895723104477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz not found in cache, downloading to /tmp/tmp_5xzvufq\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 404400730/404400730 [02:21<00:00, 2860939.71B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmp_5xzvufq to cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmp_5xzvufq\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /tmp/tmpxwcy8eoy\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 493747200\n",
      "loaders 659946496\n",
      "i_epoch 0 start: from start  95.0, from old  163.7\n",
      "epoch_start: 0 659946496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45542' class='' max='101525', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      44.86% [45542/101525 2:29:57<3:04:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "timer = ElapsedTimer()\n",
    "loss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "checkpoint_weights = np.array(checkpoint_weights) / np.sum(checkpoint_weights)\n",
    "\n",
    "dev_loss_array = np.zeros((n_seeds, TRAIN_ON_N_SPLITS, n_epochs))\n",
    "val_loss_array = np.zeros((n_seeds, TRAIN_ON_N_SPLITS, n_epochs))\n",
    "\n",
    "auc_array = np.zeros((n_seeds, TRAIN_ON_N_SPLITS, n_epochs))\n",
    "\n",
    "seed_oof_train_epoch_weighted_list = []\n",
    "seed_oof_train_last_list = []\n",
    "\n",
    "oof_train = np.zeros((n_seeds, n_epochs, len(x_train_indexed)))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1999)\n",
    "for i_seed in range(n_seeds):\n",
    "    print(f'start seed {i_seed}')\n",
    "    timer.stamp('seed start')\n",
    "    fold_dev_loss_list = []\n",
    "    fold_val_loss_list = []\n",
    "    oof_train_epoch_weighted = np.zeros(len(x_train_indexed))\n",
    "    oof_train_last = np.zeros(len(x_train_indexed))\n",
    "    \n",
    "    \n",
    "    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_train_indexed)):\n",
    "        \n",
    "        if i_fold > TRAIN_ON_N_SPLITS:\n",
    "            continue\n",
    "        timer.stamp('epoch start')\n",
    "            \n",
    "        \n",
    "        print(f'start fold {i_fold}')\n",
    "        print(f'toxic ratio dev: {y_train_torch[dev_index].mean().item()}, val: {y_train_torch[val_index].mean().item()}')\n",
    "        \n",
    "        # Load pre-trained model (weights)\n",
    "        model = NeuralNet(y_aux_train.shape[-1], sentence_feature_mat.shape[-1])\n",
    "        model.cuda()\n",
    "        print('model', torch.cuda.memory_allocated())\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        dev_sentence_feature_mat = scaler.fit_transform(sentence_feature_mat[dev_index])\n",
    "        val_sentence_feature_mat = scaler.transform(sentence_feature_mat[val_index])\n",
    "\n",
    "        dev_loader = BucketIterator([x_train_indexed[i] for i in dev_index],\n",
    "                                    torch.cat([y_train_torch[dev_index],\n",
    "                                               torch.tensor(dev_sentence_feature_mat, dtype=torch.float32).cuda()], dim=1),\n",
    "                                    batch_size=batch_size, pad_token=0, shuffle=True)\n",
    "        val_loader = BucketIterator([x_train_indexed[i] for i in val_index],\n",
    "                                    torch.cat([y_train_torch[val_index],\n",
    "                                               torch.tensor(val_sentence_feature_mat, dtype=torch.float32).cuda()], dim=1),\n",
    "                                    batch_size=batch_size, pad_token=0, shuffle=False)\n",
    "        \n",
    "        print('loaders', torch.cuda.memory_allocated())\n",
    "        \n",
    "        \n",
    "        all_test_preds = []\n",
    "        dev_loss_list = []\n",
    "        val_loss_list = []\n",
    "        weighted_val_pred = np.zeros(val_index.shape[0])\n",
    "\n",
    "        for i_epoch in range(n_epochs):\n",
    "            timer.stamp(f'i_epoch {i_epoch} start')\n",
    "            \n",
    "            print(f'epoch_start: {i_epoch}', torch.cuda.memory_allocated())\n",
    "            start_time = time.time()\n",
    "            \n",
    "            model.train()\n",
    "            dev_avg_loss = 0.\n",
    "            for i_batch, batch in enumerate(progress_bar(dev_loader)):\n",
    "                if i_epoch == 0:\n",
    "                    adjust_lr(optimizer, i_batch, min_lr=1e-6, max_lr=1e-5,\n",
    "                              n_batch_all=len(dev_loader), warm_up_batch_ratio=0.1)\n",
    "                x_batch = batch[0]\n",
    "                segment_id_batch = batch[1]\n",
    "                input_mask_batch = batch[2]\n",
    "                y_batch = batch[3]\n",
    "                index_batch = batch[4]\n",
    "                \n",
    "                y_true_batch = y_batch[:, :1+y_aux_train.shape[-1]]\n",
    "                sample_weight_batch = y_batch[:, 1+y_aux_train.shape[-1]]\n",
    "                sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n",
    "                \n",
    "                x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n",
    "#                 print('x_features', torch.cuda.memory_allocated())\n",
    "#                 timer.stamp(f'x_features')\n",
    "                \n",
    "                y_pred = model(x_features, sentence_feature_batch)\n",
    "#                 print('after_prediction', torch.cuda.memory_allocated())\n",
    "#                 timer.stamp(f'after_prediction')\n",
    "                \n",
    "                del x_features\n",
    "                torch.cuda.empty_cache()\n",
    "#                 print('torch.cuda.empty_cache()', torch.cuda.memory_allocated())\n",
    "                \n",
    "                \n",
    "                loss_fn = nn.BCEWithLogitsLoss(sample_weight_batch[:, None], reduction='sum')\n",
    "                loss = loss_fn(y_pred, y_true_batch) # last one is a sample weight\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "#                 print('loss.backward()', torch.cuda.memory_allocated())\n",
    "\n",
    "                optimizer.step()\n",
    "                dev_avg_loss += loss.item() / dev_index.shape[0]\n",
    "                \n",
    "                \n",
    "                del y_pred, loss\n",
    "                torch.cuda.empty_cache()\n",
    "#                 print('del y_pred', torch.cuda.memory_allocated())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            timer.stamp(f'after dev all batch')\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print('after dev loop', torch.cuda.memory_allocated())\n",
    "            timer.stamp(f'after all batch gc.collect()')\n",
    "            \n",
    "            dev_loss_array[i_seed, i_fold, i_epoch] = dev_avg_loss\n",
    "\n",
    "            model.eval()\n",
    "            val_avg_loss = 0.\n",
    "            epoch_val_pred = np.zeros(val_index.shape[0])\n",
    "            for batch in progress_bar(val_loader):\n",
    "                x_batch = batch[0]\n",
    "                segment_id_batch = batch[1]\n",
    "                input_mask_batch = batch[2]\n",
    "                y_batch = batch[3]\n",
    "                index_batch = batch[4]\n",
    "                \n",
    "                y_true_batch = y_batch[:, :1+y_aux_train.shape[-1]]\n",
    "                sample_weight_batch = y_batch[:, 1+y_aux_train.shape[-1]]\n",
    "                sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n",
    "                \n",
    "                x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n",
    "#                 print('x_features', torch.cuda.memory_allocated())\n",
    "#                 timer.stamp(f'x_features')\n",
    "                \n",
    "                y_pred = model(x_features, sentence_feature_batch)\n",
    "                \n",
    "#                 print('after_prediction', torch.cuda.memory_allocated())\n",
    "#                 timer.stamp(f'after_prediction')\n",
    "                \n",
    "                del x_features\n",
    "                torch.cuda.empty_cache()\n",
    "#                 print('torch.cuda.empty_cache()', torch.cuda.memory_allocated())\n",
    "                                \n",
    "                loss_fn = nn.BCEWithLogitsLoss(sample_weight_batch[:, None], reduction='sum')\n",
    "                loss = loss_fn(y_pred, y_true_batch) # last one is a sample weight\n",
    "\n",
    "                val_avg_loss += loss.item() / val_index.shape[0]\n",
    "                \n",
    "                epoch_val_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n",
    "                \n",
    "                del y_pred, loss\n",
    "                torch.cuda.empty_cache()\n",
    "#                 print('del x_cat, y_pred', torch.cuda.memory_allocated())\n",
    "            \n",
    "            timer.stamp(f'after val all batch')\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(RESULT_PATH, \n",
    "                f'seed{i_seed}-fold{i_fold}-epoch{i_epoch}.torchModelState'))\n",
    "            \n",
    "            timer.stamp(f'after model save')\n",
    "\n",
    "            val_loss_array[i_seed, i_fold, i_epoch] = val_avg_loss\n",
    "            \n",
    "            weighted_val_pred += epoch_val_pred * checkpoint_weights[i_epoch]\n",
    "            \n",
    "            oof_train[i_seed, i_epoch, val_index] = epoch_val_pred\n",
    "\n",
    "            gc.collect()\n",
    "            timer.stamp(f'after gc.collect')\n",
    "\n",
    "            \n",
    "            valid_df = train.iloc[val_index]\n",
    "            weighted_auc, overall_auc, bias_df = get_various_auc(valid_df, epoch_val_pred)\n",
    "            auc_array[i_seed, i_fold, i_epoch] = weighted_auc\n",
    "            del valid_df\n",
    "            gc.collect()\n",
    "            \n",
    "            timer.stamp(f'after gc.collect')\n",
    "\n",
    "            np.save('oof_train.npy', oof_train)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'Finished epoch {i_epoch} in {elapsed_time: .0f}, dev_loss: {dev_avg_loss:.4f}, val_loss: {val_avg_loss:.4f}' + \\\n",
    "                     f', weighted_auc: {weighted_auc}, overall_auc: {overall_auc} ',\n",
    "                  file=open(os.path.join(RESULT_PATH, RESULT_TXT), 'a'))\n",
    "        \n",
    "        timer.stamp(f'after all folds')\n",
    "        \n",
    "        oof_train_epoch_weighted[val_index] = weighted_val_pred\n",
    "        oof_train_last[val_index] = epoch_val_pred\n",
    "        \n",
    "        fold_dev_loss_list.append(dev_loss_list)\n",
    "        fold_val_loss_list.append(val_loss_list)\n",
    "        del dev_loader, val_loader, model, weighted_val_pred, epoch_val_pred, optimizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        timer.stamp(f'after all folds, gc collect')\n",
    "\n",
    "\n",
    "    seed_oof_train_epoch_weighted_list.append(oof_train_epoch_weighted)\n",
    "    seed_oof_train_last_list.append(oof_train_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
