{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from contextlib import contextmanager\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[crawl] done in 92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.9G\n",
    "with timer('crawl'):\n",
    "    crawl_emb_dict = load_embeddings(CRAWL_EMBEDDING_PATH)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.7 s, sys: 584 ms, total: 9.28 s\n",
      "Wall time: 8.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 9.9G\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "x_train = train['comment_text']\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test['comment_text']\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1804874,), (1804874,), (1804874, 6), (97320,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, y_aux_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# とりあえず100とかでいいやろう\n",
    "x_train.fillna(\"\").apply(lambda x: len(x.split())).hist(bins=100, cumulative=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(enumerable, n):\n",
    "    for i, item in enumerate(enumerable):\n",
    "        print(str(i) + '\\n' + item)\n",
    "        if i > n:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\n",
      "1\n",
      "Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\n",
      "2\n",
      "This is such an urgent design problem; kudos to you for taking it on. Very impressive!\n",
      "3\n",
      "Is this something I'll be able to install on my site? When will you be releasing it?\n",
      "4\n",
      "haha you guys are a bunch of losers.\n",
      "5\n",
      "ur a sh*tty comment.\n",
      "6\n",
      "hahahahahahahahhha suck it.\n"
     ]
    }
   ],
   "source": [
    "# このShittyとかをどうにかしたいな\n",
    "head(x_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b*tch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b9ff82ec33fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawl_emb_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b*tch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'b*tch'"
     ]
    }
   ],
   "source": [
    "crawl_emb_dict['b*tch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "haha you guys are a bunch of losers.\n",
      "1\n",
      "ur a sh*tty comment.\n",
      "2\n",
      "It's ridiculous that these guys are being called \"protesters\". Being armed is a threat of violence, which makes them terrorists.\n",
      "3\n",
      "This story gets more ridiculous by the hour! And, I love that people are sending these guys dildos in the mail now. But… if they really think there's a happy ending in this for any of them, I think they're even more deluded than all of the jokes about them assume.\n",
      "4\n",
      "Angry trolls, misogynists and Racists\", oh my. It doesn't take all of my 150 IQ to see the slant here.  it's the \"Diversity diode\" at work yet again. \"We can say anything that we want because we are Diversity. You on the other hand must only  say what we allow you to say. From now on, winning arguments against any member of diversity will be considered offensive language.  facts, cogent, linear posts and Math are now verboten.\n",
      "5\n",
      "Yet call out all Muslims for the acts of a few will get you pilloried.   So why is it okay to smear an entire religion over these few idiots?  Or is this because it's okay to bash Christian sects?\n",
      "6\n",
      "This bitch is nuts. Who would read a book by a woman.\n"
     ]
    }
   ],
   "source": [
    "head(x_train[y_train == 1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# これだと、'はembeddingに結構入ってるのに除外されちゃう。　よくないので ' だけ抜いた\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "puncts_apos = [\"''\", ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "def clean_text_apos(x: str) -> str:\n",
    "    for punct in puncts_apos:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "def clean_numbers_space(x: str) -> str:\n",
    "    x = re.sub('[0-9]{5,}', ' ##### ', x)\n",
    "    x = re.sub('[0-9]{4}', ' #### ', x)\n",
    "    x = re.sub('[0-9]{3}', ' ### ', x)\n",
    "    x = re.sub('[0-9]{2}', ' ## ', x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Dict, List\n",
    "def build_vocab(texts: pd.DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    -----\n",
    "    texts: pandas.Series\n",
    "        question textの列\n",
    "        \n",
    "    Returns\n",
    "    -----\n",
    "    dict: \n",
    "        単語とカウント\n",
    "    \n",
    "    \"\"\"\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab: Dict[str, int], embeddings_index: Dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----\n",
    "    vocab: dict\n",
    "        単語とカウント\n",
    "    embeddings_index: dict\n",
    "        load_embedの出力\n",
    "        \n",
    "    Returns:\n",
    "        list:\n",
    "            embeddingsに入ってない単語\n",
    "    \"\"\"\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / float(len(vocab))))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(float(nb_known_words) / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "l_stemmer = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    wordの編集距離1の単語のリストを返す\n",
    "    \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words, embed): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in embed)\n",
    "\n",
    "def spellcheck(word, word_rank_dict):\n",
    "    return min(known(edits1(word), word_rank_dict), key=lambda w: word_rank_dict[w])\n",
    "\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "def process_stemmer(vocab, embed):\n",
    "    \n",
    "    oov_word_set = set()\n",
    "    for word in vocab.keys():\n",
    "        vector = embed.get(word, None)\n",
    "        if vector is not None:\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.lower(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.upper(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.capitalize(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        corr_word = punct_mapping.get(word, None)\n",
    "        if corr_word is not None:\n",
    "            vector = embed.get(corr_word, None)\n",
    "            if vector is not None:\n",
    "                embed[word] = vector\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(p_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(p_stemmer.stem(word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(l_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(l_stemmer.stem(word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(s_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(s_stemmer.stem(word.decode('utf-8')), None)\n",
    "        \n",
    "        char_list = []\n",
    "        for char in word:\n",
    "            if 'LATIN LETTER SMALL CAPITAL' in char:\n",
    "                \n",
    "            \n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "        \n",
    "\n",
    "        oov_word_set.add(word)\n",
    "            \n",
    "    return embed, oov_word_set\n",
    "\n",
    "def process_spellcheck(vocab, embed, word_rank_dict, oov_set):\n",
    "    for word in vocab.keys():\n",
    "        if word not in oov_set:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(spellcheck(word, word_rank_dict), None)\n",
    "        except:\n",
    "            continue\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "    return embed\n",
    "\n",
    "def make_word_rank(embed):\n",
    "    word_rank = {}\n",
    "    for i, word in enumerate(embed):\n",
    "        word_rank[word] = i\n",
    "    return word_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_desc = pd.concat([x_train, x_test], ignore_index=True).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1902194,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 s, sys: 300 ms, total: 51.8 s\n",
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_concat_desc = concat_desc.apply(lambda x: clean_text(x))\n",
    "processed_concat_desc = processed_concat_desc.apply(lambda x: clean_numbers_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(processed_concat_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461012"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 53.78% of vocab\n",
      "Found embeddings for  98.29% of all text\n",
      "CPU times: user 358 ms, sys: 12 ms, total: 370 ms\n",
      "Wall time: 368 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oov_p = check_coverage(vocab, crawl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(oov_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##', 423979),\n",
       " ('###', 173275),\n",
       " ('####', 161039),\n",
       " ('_', 65327),\n",
       " (\"Trump's\", 25361)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_p[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 7.95 ms, total: 12.1 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_concat_desc = concat_desc.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3276"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vocab\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(processed_concat_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482096"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 53.24% of vocab\n",
      "Found embeddings for  98.91% of all text\n",
      "CPU times: user 363 ms, sys: 4.09 ms, total: 367 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oov_p = check_coverage(vocab, crawl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 65327),\n",
       " (\"Trump's\", 25361),\n",
       " (\"aren't\", 22714),\n",
       " (\"Don't\", 21779),\n",
       " (\"wouldn't\", 21158),\n",
       " (\"wasn't\", 19896),\n",
       " (\"You're\", 14954),\n",
       " (\"Let's\", 14817),\n",
       " (\"He's\", 12654),\n",
       " (\"couldn't\", 12060),\n",
       " (\"There's\", 11290),\n",
       " (\"let's\", 10409),\n",
       " (\"what's\", 10359),\n",
       " (\"shouldn't\", 10267),\n",
       " (\"hasn't\", 8539),\n",
       " (\"What's\", 8478),\n",
       " (\"Canada's\", 8409),\n",
       " (\"you've\", 8138),\n",
       " ('`', 7591),\n",
       " (\"weren't\", 6634),\n",
       " (\"Here's\", 6307),\n",
       " (\"Obama's\", 6232),\n",
       " (\"They're\", 5807),\n",
       " (\"one's\", 5607),\n",
       " (\"people's\", 5597),\n",
       " (\"you'd\", 5440),\n",
       " (\"we'll\", 5293),\n",
       " (\"they've\", 5179),\n",
       " (\"We're\", 5156),\n",
       " (\"Can't\", 4993),\n",
       " (\"they'll\", 4990),\n",
       " (\"we've\", 4944),\n",
       " (\"today's\", 4798),\n",
       " (\"Trudeau's\", 4645),\n",
       " (\"who's\", 4599),\n",
       " (\"Isn't\", 4441),\n",
       " (\"Alaska's\", 4127),\n",
       " (\"God's\", 3650),\n",
       " (\"he'll\", 3471),\n",
       " (\"ain't\", 3241),\n",
       " (\"women's\", 3182),\n",
       " (\"Didn't\", 3120),\n",
       " (\"Doesn't\", 3119),\n",
       " (\"they'd\", 3073),\n",
       " (\"She's\", 3060),\n",
       " (\"world's\", 3032),\n",
       " (\"America's\", 2962),\n",
       " (\"he'd\", 2908),\n",
       " (\"Clinton's\", 2901),\n",
       " (\"You've\", 2833),\n",
       " (\"We've\", 2758),\n",
       " (\"else's\", 2526),\n",
       " (\"Hillary's\", 2487),\n",
       " (\"we'd\", 2335),\n",
       " (\"gov't\", 2309),\n",
       " (\"country's\", 2288),\n",
       " (\"We'll\", 2278),\n",
       " (\"O'Leary\", 2190),\n",
       " (\"hadn't\", 2153),\n",
       " (\"state's\", 2145),\n",
       " (\"man's\", 2101),\n",
       " (\"government's\", 2092),\n",
       " (\"person's\", 2087),\n",
       " (\"someone's\", 2054),\n",
       " (\"Harper's\", 1946),\n",
       " ('SB21', 1936),\n",
       " (\"here's\", 1885),\n",
       " (\"nation's\", 1828),\n",
       " (\"You'll\", 1818),\n",
       " (\"They've\", 1744),\n",
       " (\"everyone's\", 1657),\n",
       " (\"Wouldn't\", 1637),\n",
       " (\"Where's\", 1632),\n",
       " (\"Hawaii's\", 1629),\n",
       " (\"Putin's\", 1573),\n",
       " (\"it'll\", 1504),\n",
       " (\"China's\", 1488),\n",
       " ('theglobeandmail', 1420),\n",
       " (\"woman's\", 1396),\n",
       " (\"You'd\", 1382),\n",
       " (\"Church's\", 1365),\n",
       " (\"Who's\", 1291),\n",
       " (\"They'll\", 1271),\n",
       " (\"i'm\", 1270),\n",
       " (\"children's\", 1258),\n",
       " (\"anyone's\", 1230),\n",
       " (\"year's\", 1229),\n",
       " (\"Hawai'i\", 1226),\n",
       " (\"60's\", 1197),\n",
       " (\"He'll\", 1184),\n",
       " (\"Shouldn't\", 1150),\n",
       " (\"70's\", 1143),\n",
       " (\"80's\", 1117),\n",
       " (\"would've\", 1116),\n",
       " (\"How's\", 1099),\n",
       " (\"Wasn't\", 1095),\n",
       " (\"Russia's\", 1091),\n",
       " (\"Oregon's\", 1085),\n",
       " (\"Aren't\", 1068),\n",
       " (\"BC's\", 1067)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_p[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.63 s, sys: 40.1 ms, total: 8.67 s\n",
      "Wall time: 8.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crawl_emb_dict, oov_glove = process_stemmer(vocab, crawl_emb_dict)\n",
    "word_rank_glove = make_word_rank(crawl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 84.33% of vocab\n",
      "Found embeddings for  99.86% of all text\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oov_g = check_coverage(vocab, process_spellcheck(vocab, crawl_emb_dict, word_rank_glove, oov_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theglobeandmail', 1420),\n",
       " ('ᴀɴᴅ', 540),\n",
       " ('nationalpost', 438),\n",
       " ('2gTbpns', 381),\n",
       " ('ᴛʜᴇ', 371),\n",
       " ('ʜᴏᴍᴇ', 358),\n",
       " ('ᴜᴘ', 357),\n",
       " ('Yᴏᴜ', 356),\n",
       " ('denverpost', 339),\n",
       " ('civilbeat', 285),\n",
       " ('RangerMC', 276),\n",
       " ('garycrum', 266),\n",
       " ('BCLibs', 260),\n",
       " ('cashapp24', 237),\n",
       " ('dailycaller', 217),\n",
       " ('washingtontimes', 210),\n",
       " ('Cheetolini', 203),\n",
       " ('Tridentinus', 201),\n",
       " ('ᴄʜᴇᴄᴋ', 194),\n",
       " ('ғᴏʀ', 191),\n",
       " ('ᴄᴏᴍᴘᴜᴛᴇʀ', 190),\n",
       " ('ᴛʜɪs', 188),\n",
       " ('ᴍᴏɴᴛʜ', 186),\n",
       " ('ᴡᴏʀᴋɪɴɢ', 185),\n",
       " ('Ontariowe', 184),\n",
       " ('ᴊᴏʙ', 184),\n",
       " ('ᴏғ', 184),\n",
       " ('ʜᴏᴜʀʟʏ', 184),\n",
       " ('ᴡᴇᴇᴋ', 180),\n",
       " ('ʟɪɴᴋ', 180),\n",
       " ('financialpost', 179),\n",
       " ('ʜᴀᴠᴇ', 179),\n",
       " ('ᴄᴀɴ', 179),\n",
       " ('MAGAphants', 178),\n",
       " ('ᴇɴᴅ', 178),\n",
       " ('ғɪʀsᴛ', 178),\n",
       " ('ʏᴏᴜʀ', 178),\n",
       " ('sɪɢɴɪɴɢ', 178),\n",
       " ('ʙᴏᴛᴛᴏᴍ', 178),\n",
       " ('ғᴏʟʟᴏᴡɪɴɢ', 178),\n",
       " ('Mᴀᴋᴇ', 178),\n",
       " ('ᴄᴏɴɴᴇᴄᴛɪᴏɴ', 178),\n",
       " ('ɪɴᴛᴇʀɴᴇᴛ', 178),\n",
       " ('ʀᴇʟɪᴀʙʟᴇ', 178),\n",
       " ('ɴᴇᴇᴅ', 178),\n",
       " ('ᴏɴʟʏ', 178),\n",
       " ('ɪɴᴄᴏᴍᴇ', 178),\n",
       " ('ᴇxᴛʀᴀ', 178),\n",
       " ('ɴᴇᴇᴅɪɴɢ', 178),\n",
       " ('ᴀɴʏᴏɴᴇ', 178),\n",
       " ('ᴍᴏᴍs', 178),\n",
       " ('sᴛᴀʏ', 178),\n",
       " ('sᴛᴜᴅᴇɴᴛs', 178),\n",
       " ('Gʀᴇᴀᴛ', 178),\n",
       " ('ғʀᴏᴍ', 178),\n",
       " ('Sᴛᴀʀᴛ', 178),\n",
       " ('Nageak', 173),\n",
       " (\"O'Leary's\", 170),\n",
       " ('scientificamerican', 161),\n",
       " ('motleycrew', 161),\n",
       " ('907AK', 158),\n",
       " ('ncronline', 158),\n",
       " ('talkingpointsmemo', 155),\n",
       " ('motherjones', 153),\n",
       " ('Outsider77', 151),\n",
       " ('Putrumpski', 150),\n",
       " ('diverdave', 148),\n",
       " ('Mahawker', 148),\n",
       " ('TheDonald', 140),\n",
       " ('antifluoridationists', 140),\n",
       " ('staradvertiser', 133),\n",
       " ('Lazeelink', 131),\n",
       " ('Pandora17', 128),\n",
       " ('22moneybay', 125),\n",
       " ('Bozievich', 125),\n",
       " ('thedailybeast', 122),\n",
       " ('covfefe', 121),\n",
       " ('RadirD', 121),\n",
       " ('skyofblue', 121),\n",
       " ('gubmut', 119),\n",
       " ('muckamuck', 118),\n",
       " ('conservativereview', 115),\n",
       " ('americanthinker', 115),\n",
       " ('vancouversun', 114),\n",
       " ('McWynnety', 114),\n",
       " ('jerry69', 113),\n",
       " ('torontosun', 111),\n",
       " ('politicususa', 109),\n",
       " ('Colkoch', 107),\n",
       " ('antifluoridationist', 107),\n",
       " (\"Gabbard's\", 107),\n",
       " ('seattletimes', 106),\n",
       " (\"ponokeali'i\", 104),\n",
       " ('pewresearch', 101),\n",
       " ('\\xadc\\xado\\xadm', 101),\n",
       " ('w\\xadw\\xadw\\xad', 97),\n",
       " ('Patkotak', 97),\n",
       " ('clickSource', 96),\n",
       " ('Whazzie', 95),\n",
       " ('jjp58', 94)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_g[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.name('ᴛ')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LATIN CAPITAL LETTER Y'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.name('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LATIN LETTER SMALL CAPITAL O'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.name('ᴏ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ᴛʜɪs'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFC', 'ᴛʜɪs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def process_stemmer(vocab, embed):\n",
    "    \n",
    "    oov_word_set = set()\n",
    "    for word in vocab.keys():\n",
    "        vector = embed.get(word, None)\n",
    "        if vector is not None:\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.lower(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.upper(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(word.capitalize(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        corr_word = punct_mapping.get(word, None)\n",
    "        if corr_word is not None:\n",
    "            vector = embed.get(corr_word, None)\n",
    "            if vector is not None:\n",
    "                embed[word] = vector\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(p_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(p_stemmer.stem(word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(l_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(l_stemmer.stem(word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(s_stemmer.stem(word), None)\n",
    "        except:\n",
    "            vector = embed.get(s_stemmer.stem(word.decode('utf-8')), None)\n",
    "                    \n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        char_list = []\n",
    "        any_small_capitial = False\n",
    "        for char in word:\n",
    "            try:\n",
    "                uni_name = unicodedata.name(char)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            if 'LATIN LETTER SMALL CAPITAL' in uni_name:\n",
    "                char = uni_name[-1]\n",
    "                any_small_capitial = True\n",
    "            if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:\n",
    "                char = 'F'\n",
    "                any_small_capitial = True\n",
    "                \n",
    "            char_list.append(char)\n",
    "            \n",
    "        if not any_small_capitial:\n",
    "            oov_word_set.add(word)\n",
    "            continue\n",
    "        \n",
    "        legit_word = ''.join(char_list)\n",
    "        \n",
    "        # 2週目\n",
    "        \n",
    "        vector = embed.get(legit_word, None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(legit_word.lower(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(legit_word.upper(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        vector = embed.get(legit_word.capitalize(), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        corr_word = punct_mapping.get(legit_word, None)\n",
    "        if corr_word is not None:\n",
    "            vector = embed.get(corr_word, None)\n",
    "            if vector is not None:\n",
    "                embed[word] = vector\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(p_stemmer.stem(legit_word), None)\n",
    "        except:\n",
    "            vector = embed.get(p_stemmer.stem(legit_word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vector = embed.get(l_stemmer.stem(legit_word), None)\n",
    "        except:\n",
    "            vector = embed.get(l_stemmer.stem(legit_word.decode('utf-8')), None)\n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            vector = embed.get(s_stemmer.stem(legit_word), None)\n",
    "        except:\n",
    "            vector = embed.get(s_stemmer.stem(legit_word.decode('utf-8')), None)\n",
    "                    \n",
    "        if vector is not None:\n",
    "            embed[word] = vector\n",
    "            continue\n",
    "\n",
    "        oov_word_set.add(word)\n",
    "            \n",
    "    return embed, oov_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 85.59% of vocab\n",
      "Found embeddings for  99.88% of all text\n",
      "CPU times: user 19.7 s, sys: 32 ms, total: 19.7 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crawl_emb_dict, oov = process_stemmer(vocab, crawl_emb_dict)\n",
    "word_rank = make_word_rank(crawl_emb_dict)\n",
    "crawl_emb_dict = process_spellcheck(vocab, crawl_emb_dict, word_rank, oov)\n",
    "oov_g = check_coverage(vocab, crawl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theglobeandmail', 1420),\n",
       " ('nationalpost', 438),\n",
       " ('2gTbpns', 381),\n",
       " ('denverpost', 339),\n",
       " ('civilbeat', 285),\n",
       " ('RangerMC', 276),\n",
       " ('garycrum', 266),\n",
       " ('cashapp24', 237),\n",
       " ('dailycaller', 217),\n",
       " ('washingtontimes', 210),\n",
       " ('Cheetolini', 203),\n",
       " ('Ontariowe', 184),\n",
       " ('financialpost', 179),\n",
       " ('MAGAphants', 178),\n",
       " ('scientificamerican', 161),\n",
       " ('motleycrew', 161),\n",
       " ('ncronline', 158),\n",
       " ('talkingpointsmemo', 155),\n",
       " ('motherjones', 153),\n",
       " ('Outsider77', 151),\n",
       " ('Putrumpski', 150),\n",
       " ('diverdave', 148),\n",
       " ('Mahawker', 148),\n",
       " ('TheDonald', 140),\n",
       " ('antifluoridationists', 140),\n",
       " ('staradvertiser', 133),\n",
       " ('Lazeelink', 131),\n",
       " ('Pandora17', 128),\n",
       " ('22moneybay', 125),\n",
       " ('Bozievich', 125),\n",
       " ('thedailybeast', 122),\n",
       " ('skyofblue', 121),\n",
       " ('muckamuck', 118),\n",
       " ('conservativereview', 115),\n",
       " ('americanthinker', 115),\n",
       " ('vancouversun', 114),\n",
       " ('McWynnety', 114),\n",
       " ('jerry69', 113),\n",
       " ('torontosun', 111),\n",
       " ('politicususa', 109),\n",
       " ('antifluoridationist', 107),\n",
       " ('seattletimes', 106),\n",
       " (\"ponokeali'i\", 104),\n",
       " ('pewresearch', 101),\n",
       " ('\\xadc\\xado\\xadm', 101),\n",
       " ('w\\xadw\\xadw\\xad', 97),\n",
       " ('Patkotak', 97),\n",
       " ('clickSource', 96),\n",
       " ('jjp58', 94),\n",
       " ('MAGAphant', 92),\n",
       " ('cn535aU5UsN7Lj8X8', 91),\n",
       " ('MNXHNMJYAz1b41', 91),\n",
       " ('1DzOz3Y6D8g', 91),\n",
       " ('crooksandliars', 91),\n",
       " ('Trumplethinskin', 90),\n",
       " ('dtrumpview', 89),\n",
       " ('factoryofincome', 89),\n",
       " ('AlwaysThere', 89),\n",
       " ('Rolovich', 87),\n",
       " ('theDonald', 86),\n",
       " ('cruxnow', 86),\n",
       " ('mcubz', 85),\n",
       " ('Trumpnuts', 85),\n",
       " ('WesternPatriot', 83),\n",
       " ('Utqiagvik', 83),\n",
       " ('3Ahomepage', 83),\n",
       " ('altrightpubs', 82),\n",
       " ('dailywire', 82),\n",
       " ('StewartBrian', 81),\n",
       " ('s\\xadp\\xado\\xadt\\xad', 81),\n",
       " ('\\xadc\\xada\\xads\\xadh\\xad', 81),\n",
       " ('Veselnitskaya', 80),\n",
       " ('nationalobserver', 80),\n",
       " ('NFLGTV', 78),\n",
       " ('libertyheadlines', 78),\n",
       " ('secondenlightenment', 76),\n",
       " ('indivisibleguide', 75),\n",
       " ('cbslocal', 75),\n",
       " ('RAILFAIL', 74),\n",
       " ('EuGreen', 74),\n",
       " ('Sniktaw', 73),\n",
       " ('Trumpski', 72),\n",
       " ('ericnorstog', 72),\n",
       " ('Eugreen', 72),\n",
       " ('bsdetection', 71),\n",
       " ('hapaguy', 70),\n",
       " ('twsrc', 69),\n",
       " ('OntariOWE', 68),\n",
       " ('realclearpolitics', 68),\n",
       " ('thegatewaypundit', 68),\n",
       " ('jobpro22', 67),\n",
       " ('LifeoftheLay', 67),\n",
       " ('oakbaystarfish', 67),\n",
       " ('xianleft', 66),\n",
       " ('thecanadianencyclopedia', 66),\n",
       " ('Rmiller101', 66),\n",
       " ('icon66', 66),\n",
       " ('BBnb7Kz', 64),\n",
       " ('disquscdn', 62),\n",
       " ('NP5491', 61)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_g[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/roov-crawl.pickle', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
